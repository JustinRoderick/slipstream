This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
app/
  favicon.ico
  globals.css
  layout.tsx
  page.tsx
components/
  LandmarkWorker.tsx
  WindowModeDemoPage.tsx
docs/
  customize.md
public/
  file.svg
  globe.svg
  next.svg
  target_visualization_mobile.vv
  target_visualization.vv
  uni05_53.ttf
  vercel.svg
  window.svg
.gitignore
demo.gif
LICENSE
next.config.ts
package.json
postcss.config.mjs
README.md
tsconfig.json
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="app/globals.css">
@import "tailwindcss";

:root {
  --background: #ffffff;
  --foreground: #171717;
}

@theme inline {
  --color-background: var(--background);
  --color-foreground: var(--foreground);
  --font-sans: var(--font-geist-sans);
  --font-mono: var(--font-geist-mono);
}

@media (prefers-color-scheme: dark) {
  :root {
    --background: #0a0a0a;
    --foreground: #ededed;
  }
}

body {
  background: var(--background);
  color: var(--foreground);
  font-family: Arial, Helvetica, sans-serif;
}

@font-face {
  font-family: 'PixelFont';
  src: url('/uni05_53.ttf') format('truetype');
  font-weight: 400;
  font-style: normal;
  font-display: swap; /* optional, improves loading */
}
</file>

<file path="app/layout.tsx">
import type { Metadata } from "next";
import { Geist, Geist_Mono } from "next/font/google";
import "./globals.css";

const geistSans = Geist({
  variable: "--font-geist-sans",
  subsets: ["latin"],
});

const geistMono = Geist_Mono({
  variable: "--font-geist-mono",
  subsets: ["latin"],
});

export const metadata: Metadata = {
  title: "Create Next App",
  description: "Generated by create next app",
};

export default function RootLayout({
  children,
}: Readonly<{
  children: React.ReactNode;
}>) {
  return (
    <html lang="en">
      <body
        className={`${geistSans.variable} ${geistMono.variable} antialiased`}
      >
        {children}
      </body>
    </html>
  );
}
</file>

<file path="app/page.tsx">
'use client';
import WindowModeDemoPage from '@/components/WindowModeDemoPage';

export default function Page() {
  return <WindowModeDemoPage />;
}
</file>

<file path="components/LandmarkWorker.tsx">
let detector: any = null;

self.onmessage = async (e) => {
  const { type, payload } = e.data;

  if (type === 'init') {
    const { wasmPath, modelPath } = payload;
    const mp = await import('@mediapipe/tasks-vision');
    const { FilesetResolver, FaceLandmarker } = mp;

    const vision = await FilesetResolver.forVisionTasks(wasmPath);
    detector = await FaceLandmarker.createFromOptions(vision, {
      baseOptions: { modelAssetPath: modelPath },
      runningMode: 'VIDEO',
      numFaces: 1,
      outputFaceBlendshapes: false,
      outputFacialTransformationMatrixes: false
    });

    self.postMessage({ type: 'ready' });
  }

  if (type === 'frame' && detector) {
    const { bitmap, timestamp } = payload;
    try {
      const res = detector.detectForVideo(bitmap, timestamp);
      self.postMessage({ type: 'landmarks', payload: res.faceLandmarks });
    } finally {
      bitmap.close();
    }
  }
};
</file>

<file path="docs/customize.md">
# Customizing Your 3D Scene

This documentation explains how to replace the default .vv files with your own 3D models and configure the camera parameters for optimal window mode experience.

## Table of Contents

- [Converting 3D Models to .vv Format](#converting-3d-models-to-vv-format)
- [Replacing .vv Files](#replacing-vv-files)
- [Camera Configuration Parameters](#camera-configuration-parameters)
- [Understanding the Portal Effect](#understanding-the-portal-effect)

## Converting 3D Models to .vv Format

### GLB to VV Converter

The easiest way to convert your 3D models is using the official GLB to VV converter:

**[Convert GLB to VV →](https://www.splats.com/tools/voxelize)**

**Requirements:**
- **File format**: GLB files only
- **File size limit**: 500MB maximum
- **Model type**: Static 3D models (no animations)

**Process:**
1. Upload your GLB file to the converter
2. Wait for conversion to complete
3. Download the generated .vv file


## Replacing .vv Files

The demo uses two .vv files for different orientations:

- `public/target_visualization.vv` - Desktop/landscape version
- `public/target_visualization_mobile.vv` - Mobile/portrait version

### Steps to Replace:

1. **Convert your 3D model** to .vv format using the converter above
2. **Replace the existing files** in the `public/` directory:
   ```bash
   # Replace desktop version
   cp your_model.vv public/target_visualization.vv
   
   # Replace mobile version (optional - can use same file)
   cp your_model.vv public/target_visualization_mobile.vv
   ```
3. **Restart the development server** to see changes

### File Path Configuration

If you want to use different file names, update the paths in `components/WindowModeDemoPage.tsx`:

```typescript
const vvUrl = isPortrait
  ? "/your_mobile_model.vv"    // Change this
  : "/your_desktop_model.vv";  // Change this
```

## Camera Configuration Parameters

These parameters control how the 3D scene responds to your head movement. Modify them in `components/WindowModeDemoPage.tsx`:

### WORLD_TO_VOXEL_SCALE

```typescript
const WORLD_TO_VOXEL_SCALE = 0.0075;
```

**Purpose**: Converts real-world units (centimeters) to voxel space units.

**Effect**: 
- **Higher values** = More exaggerated head movement response
- **Lower values** = Subtler head movement response

**Typical range**: 0.001 - 0.02

### SCREEN_SCALE

```typescript
const SCREEN_SCALE = 0.2 * 1.684;
```

**Purpose**: Determines how large the "window" appears in virtual space.

**Effect**:
- **Higher values** = Larger window, more immersive effect
- **Lower values** = Smaller window, more focused view

**Typical range**: 0.1 - 0.5

### SCREEN_POSITION

```typescript
const SCREEN_POSITION = [0.0, 0.0, -0.5];
```

**Purpose**: Where the screen is positioned in 3D voxel space.

**Format**: `[x, y, z]` coordinates

**Effect**:
- **X axis**: Left/right screen position
- **Y axis**: Up/down screen position  
- **Z axis**: Forward/back screen position (negative = closer to viewer)

**Typical range**: 
- X: -1.0 to 1.0
- Y: -1.0 to 1.0
- Z: -2.0 to 0.0

### SCREEN_TARGET

```typescript
const SCREEN_TARGET = [0.0, 0.0, 0.0];
```

**Purpose**: Where the screen is looking towards in 3D space.

**Format**: `[x, y, z]` coordinates

**Effect**: Controls the initial viewing direction of your 3D scene.

**Common values**:
- `[0.0, 0.0, 0.0]` - Looking at the center
- `[0.0, 0.0, 1.0]` - Looking forward
- `[0.0, 1.0, 0.0]` - Looking up

## Understanding the Portal Effect

The window mode creates a "portal" effect by using an **off-axis projection matrix**. This technique:

1. **Tracks your head position** relative to the screen
2. **Calculates your eye position** in 3D space
3. **Renders the scene** from your eye's perspective
4. **Creates the illusion** that the 3D scene exists behind the screen

### Camera Position Calculation

The system automatically calculates your eye position using:

```typescript
// Your eye position is calculated from head tracking
let avgPos = [
  (irisPosRight.x + irisPosLeft.x) / 2.0,
  (irisPosRight.y + irisPosLeft.y) / 2.0,
  (irisPosRight.z + irisPosLeft.z) / 2.0
];

// Applied to the camera
(vvRef.current as any).setCamera('portal', {
  eyePosWorld: avgPos,           // Your calculated eye position
  screenScale: SCREEN_SCALE,     // Window size
  worldToVoxelScale: WORLD_TO_VOXEL_SCALE,  // Movement sensitivity
  screenPos: SCREEN_POSITION,    // Screen location in 3D space
  screenTarget: SCREEN_TARGET    // Screen viewing direction
});
```

### Optimization Tips

- **Start with default values** and adjust gradually
- **Test with different head positions** to ensure smooth tracking
- **Consider your 3D model's scale** when setting WORLD_TO_VOXEL_SCALE
- **Adjust SCREEN_POSITION** to center your model in the viewport

---

*For general project information, see the [README](../README.md).*
</file>

<file path="public/file.svg">
<svg fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><path d="M14.5 13.5V5.41a1 1 0 0 0-.3-.7L9.8.29A1 1 0 0 0 9.08 0H1.5v13.5A2.5 2.5 0 0 0 4 16h8a2.5 2.5 0 0 0 2.5-2.5m-1.5 0v-7H8v-5H3v12a1 1 0 0 0 1 1h8a1 1 0 0 0 1-1M9.5 5V2.12L12.38 5zM5.13 5h-.62v1.25h2.12V5zm-.62 3h7.12v1.25H4.5zm.62 3h-.62v1.25h7.12V11z" clip-rule="evenodd" fill="#666" fill-rule="evenodd"/></svg>
</file>

<file path="public/globe.svg">
<svg fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><g clip-path="url(#a)"><path fill-rule="evenodd" clip-rule="evenodd" d="M10.27 14.1a6.5 6.5 0 0 0 3.67-3.45q-1.24.21-2.7.34-.31 1.83-.97 3.1M8 16A8 8 0 1 0 8 0a8 8 0 0 0 0 16m.48-1.52a7 7 0 0 1-.96 0H7.5a4 4 0 0 1-.84-1.32q-.38-.89-.63-2.08a40 40 0 0 0 3.92 0q-.25 1.2-.63 2.08a4 4 0 0 1-.84 1.31zm2.94-4.76q1.66-.15 2.95-.43a7 7 0 0 0 0-2.58q-1.3-.27-2.95-.43a18 18 0 0 1 0 3.44m-1.27-3.54a17 17 0 0 1 0 3.64 39 39 0 0 1-4.3 0 17 17 0 0 1 0-3.64 39 39 0 0 1 4.3 0m1.1-1.17q1.45.13 2.69.34a6.5 6.5 0 0 0-3.67-3.44q.65 1.26.98 3.1M8.48 1.5l.01.02q.41.37.84 1.31.38.89.63 2.08a40 40 0 0 0-3.92 0q.25-1.2.63-2.08a4 4 0 0 1 .85-1.32 7 7 0 0 1 .96 0m-2.75.4a6.5 6.5 0 0 0-3.67 3.44 29 29 0 0 1 2.7-.34q.31-1.83.97-3.1M4.58 6.28q-1.66.16-2.95.43a7 7 0 0 0 0 2.58q1.3.27 2.95.43a18 18 0 0 1 0-3.44m.17 4.71q-1.45-.12-2.69-.34a6.5 6.5 0 0 0 3.67 3.44q-.65-1.27-.98-3.1" fill="#666"/></g><defs><clipPath id="a"><path fill="#fff" d="M0 0h16v16H0z"/></clipPath></defs></svg>
</file>

<file path="public/next.svg">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 394 80"><path fill="#000" d="M262 0h68.5v12.7h-27.2v66.6h-13.6V12.7H262V0ZM149 0v12.7H94v20.4h44.3v12.6H94v21h55v12.6H80.5V0h68.7zm34.3 0h-17.8l63.8 79.4h17.9l-32-39.7 32-39.6h-17.9l-23 28.6-23-28.6zm18.3 56.7-9-11-27.1 33.7h17.8l18.3-22.7z"/><path fill="#000" d="M81 79.3 17 0H0v79.3h13.6V17l50.2 62.3H81Zm252.6-.4c-1 0-1.8-.4-2.5-1s-1.1-1.6-1.1-2.6.3-1.8 1-2.5 1.6-1 2.6-1 1.8.3 2.5 1a3.4 3.4 0 0 1 .6 4.3 3.7 3.7 0 0 1-3 1.8zm23.2-33.5h6v23.3c0 2.1-.4 4-1.3 5.5a9.1 9.1 0 0 1-3.8 3.5c-1.6.8-3.5 1.3-5.7 1.3-2 0-3.7-.4-5.3-1s-2.8-1.8-3.7-3.2c-.9-1.3-1.4-3-1.4-5h6c.1.8.3 1.6.7 2.2s1 1.2 1.6 1.5c.7.4 1.5.5 2.4.5 1 0 1.8-.2 2.4-.6a4 4 0 0 0 1.6-1.8c.3-.8.5-1.8.5-3V45.5zm30.9 9.1a4.4 4.4 0 0 0-2-3.3 7.5 7.5 0 0 0-4.3-1.1c-1.3 0-2.4.2-3.3.5-.9.4-1.6 1-2 1.6a3.5 3.5 0 0 0-.3 4c.3.5.7.9 1.3 1.2l1.8 1 2 .5 3.2.8c1.3.3 2.5.7 3.7 1.2a13 13 0 0 1 3.2 1.8 8.1 8.1 0 0 1 3 6.5c0 2-.5 3.7-1.5 5.1a10 10 0 0 1-4.4 3.5c-1.8.8-4.1 1.2-6.8 1.2-2.6 0-4.9-.4-6.8-1.2-2-.8-3.4-2-4.5-3.5a10 10 0 0 1-1.7-5.6h6a5 5 0 0 0 3.5 4.6c1 .4 2.2.6 3.4.6 1.3 0 2.5-.2 3.5-.6 1-.4 1.8-1 2.4-1.7a4 4 0 0 0 .8-2.4c0-.9-.2-1.6-.7-2.2a11 11 0 0 0-2.1-1.4l-3.2-1-3.8-1c-2.8-.7-5-1.7-6.6-3.2a7.2 7.2 0 0 1-2.4-5.7 8 8 0 0 1 1.7-5 10 10 0 0 1 4.3-3.5c2-.8 4-1.2 6.4-1.2 2.3 0 4.4.4 6.2 1.2 1.8.8 3.2 2 4.3 3.4 1 1.4 1.5 3 1.5 5h-5.8z"/></svg>
</file>

<file path="public/vercel.svg">
<svg fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1155 1000"><path d="m577.3 0 577.4 1000H0z" fill="#fff"/></svg>
</file>

<file path="public/window.svg">
<svg fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path fill-rule="evenodd" clip-rule="evenodd" d="M1.5 2.5h13v10a1 1 0 0 1-1 1h-11a1 1 0 0 1-1-1zM0 1h16v11.5a2.5 2.5 0 0 1-2.5 2.5h-11A2.5 2.5 0 0 1 0 12.5zm3.75 4.5a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5M7 4.75a.75.75 0 1 1-1.5 0 .75.75 0 0 1 1.5 0m1.75.75a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5" fill="#666"/></svg>
</file>

<file path=".gitignore">
# See https://help.github.com/articles/ignoring-files/ for more about ignoring files.

# dependencies
/node_modules
/.pnp
.pnp.*
.yarn/*
!.yarn/patches
!.yarn/plugins
!.yarn/releases
!.yarn/versions

# testing
/coverage

# next.js
/.next/
/out/

# production
/build

# misc
.DS_Store
*.pem

# debug
npm-debug.log*
yarn-debug.log*
yarn-error.log*
.pnpm-debug.log*

# env files (can opt-in for committing if needed)
.env*

# vercel
.vercel

# typescript
*.tsbuildinfo
next-env.d.ts
</file>

<file path="LICENSE">
MIT License

Copyright (c) 2025 True3D Technologies, Inc

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="next.config.ts">
import type { NextConfig } from "next";

const nextConfig: NextConfig = {
  async headers() { // these headers are needed for WASM multithreading (which spatial-player uses)
    return [
      {
        source: "/:path*",
        headers: [
          {
            key: "Cross-Origin-Opener-Policy",
            value: "same-origin",
          },
          {
            key: "Cross-Origin-Embedder-Policy",
            value: "require-corp",
          },
        ],
      },
    ];
  },
};

export default nextConfig;
</file>

<file path="package.json">
{
  "name": "window-mode",
  "version": "0.1.0",
  "private": true,
  "scripts": {
    "dev": "next dev",
    "build": "next build",
    "start": "next start"
  },
  "dependencies": {
    "@mediapipe/tasks-vision": "^0.10.22-rc.20250304",
    "lucide-react": "^0.544.0",
    "next": "15.5.4",
    "react": "19.1.0",
    "react-dom": "19.1.0",
    "spatial-player": "^3.2.13"
  },
  "devDependencies": {
    "@tailwindcss/postcss": "^4",
    "@types/node": "^20",
    "@types/react": "^19",
    "@types/react-dom": "^19",
    "tailwindcss": "^4",
    "typescript": "^5"
  }
}
</file>

<file path="postcss.config.mjs">
const config = {
  plugins: ["@tailwindcss/postcss"],
};

export default config;
</file>

<file path="tsconfig.json">
{
  "compilerOptions": {
    "target": "ES2017",
    "lib": ["dom", "dom.iterable", "esnext"],
    "allowJs": true,
    "skipLibCheck": true,
    "strict": true,
    "noEmit": true,
    "esModuleInterop": true,
    "module": "esnext",
    "moduleResolution": "bundler",
    "resolveJsonModule": true,
    "isolatedModules": true,
    "jsx": "preserve",
    "incremental": true,
    "plugins": [
      {
        "name": "next"
      }
    ],
    "paths": {
      "@/*": ["./*"]
    }
  },
  "include": ["next-env.d.ts", "**/*.ts", "**/*.tsx", ".next/types/**/*.ts"],
  "exclude": ["node_modules"]
}
</file>

<file path="components/WindowModeDemoPage.tsx">
'use client';

import { useEffect, useRef, useState } from 'react';
import { ScanFace } from 'lucide-react';

// -------------------------------------------------- //

/**
 * a 2D coordinate
 */
type Pt = { x: number; y: number };

/**
 * 2D landmarks corresponding to a single iris, in image coordinates
 */
type Iris = {
  center: Pt,
  edges: Pt[]
};

/**
 * a typical laptop webcam FOV
 * TODO: can we query this?
 */
const DEFAULT_HFOV_DEG = 60;

/**
 * these values get passed to spatial-player and determine
 * how exagerrated the scene moves with your head
 * 
 * - worldToVoxelScale converts world-space units (cm, ft, etc) to voxels
 * - screenScale determines how large the "window" is in virtual space. The voxel volume always occupies [(-1,-1,-1), (1,1,1)]
 * - screenPos determines where in voxel space the screen is located
 * - screenTarget determines where in voxel space the screen looks towards
 */
const WORLD_TO_VOXEL_SCALE = 0.0075;
const SCREEN_SCALE = 0.2 * 1.684;
const SCREEN_POSITION = [0.0, 0.0, -0.5];
const SCREEN_TARGET = [0.0, 0.0, 0.0];

/**
 * the FaceLandmarker indices for the left and right irises
 * from https://github.com/google-ai-edge/mediapipe/blob/master/docs/solutions/iris.md#ml-pipeline
 */
const RIGHT_IRIS_IDX = 468;
const LEFT_IRIS_IDX = 473;

// -------------------------------------------------- //

export default function WindowModeDemoPage() {

  // ------------------ //
  // STATE:

  const isPortrait = useIsPortrait();
  const isWebGPUSupported = (navigator as any).gpu != null;
  const vvUrl = isPortrait
    ? "/target_visualization_mobile.vv"
    : "/target_visualization.vv";

  const [error, setError] = useState<string | null>(null);
  const [numFramesFaceHidden, setNumFramesFaceHidden] = useState(0);
  const [hasPermission, setHasPermission] = useState<boolean>(false);
  const [isRequestingPermission, setIsRequestingPermission] = useState<boolean>(false);
  const [showTiltInstruction, setShowTiltInstruction] = useState<boolean>(false);

  const vvRef = useRef<HTMLDivElement | null>(null);
  const videoRef  = useRef<HTMLVideoElement | null>(null);

  const irisDistRightRef = useRef<number | null>(null);
  const irisDistLeftRef  = useRef<number | null>(null);

  const isPortraitRef = useRef(isPortrait);
  const numFramesFaceHiddenRef = useRef(numFramesFaceHidden);

  // ------------------ //
  // UTILITY FUNCTION:

  /**
   * sets a cookie
   */
  const setCookie = (name: string, value: string, days: number = 365) => {
    const expires = new Date();
    expires.setTime(expires.getTime() + (days * 24 * 60 * 60 * 1000));
    document.cookie = `${name}=${value};expires=${expires.toUTCString()};path=/;SameSite=Lax`;
  };

  /**
   * retrieves a cookie
   */
  const getCookie = (name: string): string | null => {
    const nameEQ = name + "=";
    const ca = document.cookie.split(';');
    for (let i = 0; i < ca.length; i++) {
      let c = ca[i];
      while (c.charAt(0) === ' ') c = c.substring(1, c.length);
      if (c.indexOf(nameEQ) === 0) return c.substring(nameEQ.length, c.length);
    }

    return null;
  };

  /**
   * request and caches camera permissions
   */
  const requestCameraPermission = async () => {
    setIsRequestingPermission(true);
    
    try {
      const stream = await navigator.mediaDevices.getUserMedia({
        video: {
          facingMode: "user",
          width: { ideal: 160 },
          height: { ideal: 120 }
        },
        audio: false
      });
      
      // Stop the stream immediately after getting permission
      stream.getTracks().forEach(track => track.stop());
      
      // Save permission to cookie for future visits
      setCookie('camera_permission_granted', 'true', 365);
      setHasPermission(true);
      
    } catch (e: any) {
      console.error('Camera permission denied:', e);
      setError('Camera access is required for this experience. Please allow camera access and refresh the page.');
      
    } finally {
      setIsRequestingPermission(false);
    }
  };

  /**
   * returns the focal length given the horizontal FOV
   */
  const focalLengthPixels = (imageWidthPx: number, hFovDeg: number) => {
    const a = (hFovDeg * Math.PI) / 180;
    return imageWidthPx / (2 * Math.tan(a / 2));
  }

  // ------------------ //
  // useEffects:

  /**
   * imports spatial-player
   * spatial-player uses top-level async/await so we need to import dynamically
   */
  useEffect(() => {
    import('spatial-player/src/index.js' as any)
  }, []);

  /**
   * updates isPortraitRef
   */
  useEffect(() => {
    isPortraitRef.current = isPortrait;
  }, [isPortrait]);

  /**
   * checks for existing 
   */
  useEffect(() => {
    const savedPermission = getCookie('camera_permission_granted');
    if (savedPermission === 'true') {
      setHasPermission(true);
    }
  }, []);

  /**
   * shows instructions
   */
  useEffect(() => {
    if (!hasPermission) return;
    
    setShowTiltInstruction(true);
    
    const hideTiltInstructionTimer = setTimeout(() => {
      setShowTiltInstruction(false);
    }, 3000); // 3 seconds

    return () => {
      clearTimeout(hideTiltInstructionTimer);
    };
  }, [hasPermission]);

  /**
   * updates numFramesFaceHiddenRef
   */
  useEffect(() => {
    numFramesFaceHiddenRef.current = numFramesFaceHidden;
  }, [numFramesFaceHidden]);

  /**
   * main initialization + loop
   */
  useEffect(() => {
    if (!hasPermission) return;

    let running = true;
    let worker: Worker;

    async function init() {
      try {

        //get camera:
        //-----------------
        const stream = await navigator.mediaDevices.getUserMedia({
          video: {
            facingMode: "user",
            width: { ideal: 160 },
            height: { ideal: 120 }
          },
          audio: false
        });
        const video = videoRef.current!;
        video.srcObject = stream;
        await video.play();

        //spawn facelandmarker worker:
        //we do landmarking in a worker so we don't block rendering on the main thread
        //-----------------
        worker = new Worker(new URL('./LandmarkWorker.tsx', import.meta.url), {
          type: 'module'
        });

        worker.postMessage({
          type: 'init',
          payload: {
            wasmPath: 'https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest/wasm',
            modelPath: 'https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task'
          }
        });

        let lastTime = -1;

        let landmarkingReady = false;
        let landmarkingInFlight = false;
        let lastVideoTime = -1;
        let latestLandmarks: any[] | null = null;

        worker.onmessage = (e) => {
          if (e.data.type === 'landmarks') {
            latestLandmarks = e.data.payload?.[0] ?? null;
            landmarkingInFlight = false;

            if(latestLandmarks)
              setNumFramesFaceHidden(0);
            else
              setNumFramesFaceHidden(numFramesFaceHiddenRef.current + 1);
          }

          if(e.data.type === 'ready')
            landmarkingReady = true;
        };

        //define helpers:
        //-----------------

        //reads an iris from the mediapipe landmarks
        function extractIris(landmarks: any[], idx: number): Iris {
          let edges = [];
          for (let i = 0; i < 4; i++) {
            let landmark = landmarks[idx + 1 + i];
            edges.push({ x: landmark.x, y: landmark.y });
          }

          return {
            center: { x: landmarks[idx].x, y: landmarks[idx].y },
            edges
          };
        }

        //computes the distance from the webcam to the iris
        //uses the fact that the human iris has a relatively fixed size, regardless of age/genetics
        function irisDistance(iris: Iris, hFovDeg = DEFAULT_HFOV_DEG): number {
          const IRIS_DIAMETER_MM = 11.7; //average human iris size

          let dx = ((iris.edges[0].x - iris.edges[2].x) + (iris.edges[1].x - iris.edges[3].x)) 
            / 2.0 * video.videoWidth;
          let dy = ((iris.edges[0].y - iris.edges[2].y) + (iris.edges[1].y - iris.edges[3].y)) 
            / 2.0 * video.videoHeight;

          let irisSize = Math.sqrt(dx * dx + dy * dy);

          const fpx = focalLengthPixels(video.videoWidth, hFovDeg);

          const irisDiamCm = IRIS_DIAMETER_MM / 10;
          return (fpx * irisDiamCm) / irisSize;
        }

        //uses the distance + screen position of the iris to compute its metric position, relative to the webcam
        function irisPosition(iris: Iris, distanceCm: number, hFovDeg = DEFAULT_HFOV_DEG): { x: number; y: number; z: number } {
          const W = video.videoWidth;
          const H = video.videoHeight;

          const fpx = focalLengthPixels(W, hFovDeg);

          const u = iris.center.x;
          const v = iris.center.y;

          const x = -(u * W - W / 2) * distanceCm / fpx;
          const y = -(v * H - H / 2) * distanceCm / fpx;
          const z = distanceCm;

          return { x, y, z };
        }

        //define main loop: 
        //-----------------
        function loop() {
          if (!running)
            return;

          const currentTime = performance.now();
          const dt = currentTime - lastTime;

          lastTime = currentTime;

          //send video frame to worker
          if(landmarkingReady && !landmarkingInFlight && video.currentTime !== lastVideoTime) {
            const videoTimestamp = Math.round(video.currentTime * 1000);
            createImageBitmap(video).then((bitmap) => {
              worker.postMessage({ type: 'frame', payload: { bitmap, timestamp: videoTimestamp } }, [bitmap]);
            });

            landmarkingInFlight = true;
            lastVideoTime = video.currentTime;
          }

          if (latestLandmarks) {

            //extract irises
            const irisRight = extractIris(latestLandmarks, RIGHT_IRIS_IDX);
            const irisLeft = extractIris(latestLandmarks, LEFT_IRIS_IDX);

            //compute distances
            
            const irisTargetDistRight = irisDistance(irisRight);
            const irisTargetDistLeft = irisDistance(irisLeft);
            
            var irisDistRight = irisDistRightRef.current;
            var irisDistLeft  = irisDistLeftRef.current;

            //update current distance
            //the distance estimation is pretty noisy, so we do this to smooth it out
            const distanceDecay = 1.0 - Math.pow(0.99, dt);

            irisDistRight = irisDistRight != null
              ? irisDistRight + (irisTargetDistRight - irisDistRight) * distanceDecay
              : irisTargetDistRight;

            irisDistLeft = irisDistLeft != null
              ? irisDistLeft + (irisTargetDistLeft - irisDistLeft) * distanceDecay
              : irisTargetDistLeft;

            irisDistRightRef.current = irisDistRight;
            irisDistLeftRef.current = irisDistLeft;

            const minDist = Math.min(irisDistLeft, irisDistRight);

            //compute positions
            let irisPosRight = irisPosition(irisRight, minDist);
            let irisPosLeft = irisPosition(irisLeft, minDist);

            //update vv camera
            //.vv (voxel volume) is our format for 3D voxel scenes
            //spatial-player has utilties for rendering them
            if (customElements.get('vv-player')) {
              let avgPos = [
                (irisPosRight.x + irisPosLeft.x) / 2.0,
                (irisPosRight.y + irisPosLeft.y) / 2.0,
                (irisPosRight.z + irisPosLeft.z) / 2.0
              ];

              //do some jank manual correction so its more aligned
              //TODO: fix this
              avgPos[1] -= isPortraitRef.current ? 30.0 : 20.0;

              //to achieve the "window" effect, we use spatial-player's builtin
              //"portal" camera mode. this computes an off-axis projection matrix, and uses that
              //to render the scene in 3D

              //spatial-player is not yet open source, but the projection matrix is computed 
              //with the standard off-axis projection formula. for an overview of this, see
              // https://en.wikibooks.org/wiki/Cg_Programming/Unity/Projection_for_Virtual_Reality

              (vvRef.current as any).setCamera('portal', {
                eyePosWorld: avgPos,
                screenScale: SCREEN_SCALE,
                worldToVoxelScale: WORLD_TO_VOXEL_SCALE,

                screenPos: SCREEN_POSITION,
                screenTarget: SCREEN_TARGET
              });
            }
          }

          requestAnimationFrame(loop);
        }

        //start main loop:
        //-----------------
        requestAnimationFrame(loop);
      }
      catch (e: any) {
        console.error(e);
        setError(e?.message ?? 'Failed to initialize');
      }
    }

    //init: 
    //-----------------
    init();

    return () => {
      running = false;
      worker?.terminate();
      const v = videoRef.current;
      const stream = v && (v.srcObject as MediaStream);
      stream?.getTracks()?.forEach(t => t.stop());
    };
  }, [hasPermission]);

  /**
   * determines whether we are in portrait or landscale
   * orientation, used to render the appropriate .vv
   * (a .vv is a voxel volume file, stores a 3D scene)
   */
  function useIsPortrait() {
    const [isPortrait, setIsPortrait] = useState(false);

    useEffect(() => {
      const checkOrientation: any = () => {
        if (typeof window !== 'undefined') {
          setIsPortrait(window.innerHeight > window.innerWidth);
        }
      };

      checkOrientation();

      window.addEventListener('resize', checkOrientation);
      return () => {
        window.removeEventListener('resize', checkOrientation);
      };
    }, []);

    return isPortrait;
  }

  // ------------------ //
  // LAYOUT:

  return (
    <main style={{ 
      // display: 'grid', fontFamily: 'system-ui, sans-serif', backgroundColor: 'white' 
      }}
      
      className="min-h-screen bg-black flex flex-col items-center justify-center">
      
      {/* Permission Request Screen */}
      {!hasPermission && (
        <div 
          className="absolute inset-0 flex flex-col items-center justify-center z-50"
          style={{
            backgroundImage: 'url(/target_demo.png)',
            backgroundSize: 'cover',
            backgroundPosition: 'center',
            backgroundRepeat: 'no-repeat'
          }}
        >
          {/* Faded overlay */}
          <div className="absolute inset-0 bg-black/70"></div>
          <div className="relative z-10 max-w-md mx-auto text-center px-6">
            {/* ScanFace Icon */}
            <div className="mb-8">
              <div className="w-24 h-24 mx-auto bg-white/10 rounded-full flex items-center justify-center backdrop-blur-sm border border-white/20">
                <ScanFace className="w-12 h-12 text-white" />
              </div>
            </div>

            {/* Title */}
            <h1 className="text-3xl font-bold text-white mb-4">
              3D Viewer Demo
            </h1>

            {/* Description */}
            <p className="text-lg text-gray-300 mb-8 leading-relaxed">
            We use head tracking to enhance this experience. It allows the 3D scene to react naturally to your movements. Try tilting your head to see how the perspective shifts. It is designed for a single viewer.</p>

            {/* Permission Button */}
            <button
              onClick={requestCameraPermission}
              disabled={isRequestingPermission}
              className="w-full bg-white text-black font-semibold py-4 px-8 rounded-lg hover:bg-gray-100 disabled:opacity-50 disabled:cursor-not-allowed transition-all duration-200 text-lg"
            >
              {isRequestingPermission ? (
                <div className="flex items-center justify-center gap-3">
                  <div className="w-5 h-5 border-2 border-black border-t-transparent rounded-full animate-spin"></div>
                  Requesting Access...
                </div>
              ) : (
                'Allow Camera Access'
              )}
            </button>

            {/* Privacy Note */}
            <p className="text-sm text-gray-400 mt-6 leading-relaxed">
              Your data is processed locally on your device and is not stored or transmitted anywhere.
            </p>
          </div>
        </div>
      )}

      {/* Main content - only show when permission is granted */}
      {hasPermission && (
        <>
          {/* Information icon with tooltip */}
          <div className="absolute top-4 left-4 z-50">
        <div className="relative group">
          <div className="w-8 h-8 bg-black/60 hover:bg-black/80 text-white rounded-full flex items-center justify-center cursor-help transition-colors duration-200 backdrop-blur-sm border border-white/20">
            <span className="text-lg font-bold italic">i</span>
          </div>
          
          {/* Tooltip */}
          <div className="absolute left-10 top-0 w-80 bg-black/90 text-white p-4 rounded-lg text-sm opacity-0 group-hover:opacity-100 transition-opacity duration-200 pointer-events-none backdrop-blur-sm border border-white/20 shadow-lg">
            <div className="font-semibold mb-2">3D Viewer Demo</div>
            <div className="text-gray-200 leading-relaxed">
            This demo uses your camera to track your head in real time. We map your head position to 3D camera controls so the video feels immersive and responsive. It is designed and recommended for a single viewer.            </div>
          </div>
        </div>
      </div>

      <div style={{ position: 'relative', width: 'min(100%, 720px)' }}>
        <video ref={videoRef} playsInline muted style={{ width: '100%', height: 'auto' }} className='hidden' />
      </div>

      <div
        className="w-full h-full bg-black flex items-center justify-center border-4 rounded-lg"
        style={{ borderColor: "#333" }}
      >
        <div className={`relative bg-black rounded-lg overflow-hidden ${isPortrait ? 'aspect-[9/16]' : 'aspect-[16/9]'}`} style={{ width: '100%', height: '100%', maxWidth: '100vw', maxHeight: '100vh' }}>
          {!isWebGPUSupported ? (
            // WebGPU not supported - show error message
            <div className="flex items-center justify-center w-full h-full">
              <div
                style={{
                  position: 'absolute',
                  top: '50%',
                  left: '50%',
                  transform: 'translate(-50%, -50%)',
                  padding: '1.5rem 3rem',
                  backgroundColor: 'rgba(0, 0, 0, 0.85)',
                  border: '2px solid rgba(112, 112, 112, 0.5)',
                  borderRadius: '12px',
                  color: 'white',
                  fontFamily: 'PixelFont',
                  fontSize: '2rem',
                  fontWeight: 700,
                  textAlign: 'center',
                  textShadow: '0 0 8px rgba(0,0,0,0.6)',
                  pointerEvents: 'none',
                }}
              >
                WebGPU is not supported on your browser
              </div>
            </div>
          ) : (
            // WebGPU supported - show the player
            <>
              {/* @ts-expect-error - vv-player is a custom element from spatial-player */}
              <vv-player
                ref={vvRef}
                src={vvUrl}
                bounding-box="hide"
                top-color="0 0 0 1"
                bot-color="0 0 0 1"
                video-controls="hide"
                style={{ width: '100%', height: '100%', display: 'block' }}
              />
            </>
          )}
        </div>
      </div>

      {numFramesFaceHidden > 3 && (
        <>
          {/* Red edge overlay */}
          <div
            style={{
              position: 'absolute',
              inset: 0,
              pointerEvents: 'none',
              borderRadius: 'inherit',
              background: `
                linear-gradient(to bottom, rgba(255, 100, 103, 0.2) 0%, transparent 100%) top,
                linear-gradient(to top,    rgba(255, 100, 103, 0.2) 0%, transparent 100%) bottom,
                linear-gradient(to right,  rgba(255, 100, 103, 0.2) 0%, transparent 100%) left,
                linear-gradient(to left,   rgba(255, 100, 103, 0.2) 0%, transparent 100%) right
              `,
              backgroundRepeat: 'no-repeat',
              backgroundSize: `${window.innerWidth}px ${window.innerHeight * 0.2}px, 
                              ${window.innerWidth}px ${window.innerHeight * 0.2}px, 
                              ${window.innerWidth * 0.2}px ${window.innerHeight}px, 
                              ${window.innerWidth * 0.2}px ${window.innerHeight}px`,
              transition: 'opacity 0.3s',
            }}
          />

          <div
            style={{
              position: 'absolute',
              top: '50%',
              left: '50%',
              transform: 'translate(-50%, -50%)',
              padding: '1.5rem 3rem',
              backgroundColor: 'rgba(0, 0, 0, 0.85)',
              border: '2px solid rgba(112, 112, 112, 0.5)',
              borderRadius: '12px',
              color: 'white',
              fontFamily: 'PixelFont',
              fontSize: '2rem',
              fontWeight: 700,
              textAlign: 'center',
              textShadow: '0 0 8px rgba(0,0,0,0.6)',
              pointerEvents: 'none',
            }}
          >
            CAN&apos;T FIND USER
            <div
              style={{
                marginTop: '0.5rem',
                fontSize: '1.2rem',
                fontWeight: 400,
                color: 'rgba(255,255,255,0.9)',
              }}
            >
              Please center your face in the camera frame
            </div>
          </div>
        </>
      )}
      
      {/* Tilt instruction popup - dismisses on head movement or after 8s */}
      {showTiltInstruction && isWebGPUSupported && (
        <div className="absolute inset-0 z-50 flex items-end justify-center pb-16 pointer-events-none">
          <div className="bg-white/10 text-white px-6 py-4 rounded-xl text-lg font-medium backdrop-blur-md border border-white/30 shadow-lg transition-opacity duration-500">
            <span>Tilt your head</span>
          </div>
        </div>
      )}
      
      {error && <div style={{ color: 'crimson' }}>{error}</div>}
        </>
      )}
    </main>
  );
}
</file>

<file path="README.md">
# Window Mode
This is a demo of True3D labs' "window mode". Check out the live demo [here](https://lab.true3d.com/targets). 

![Demo](demo.gif)

## Getting Started

To run this project locally:

```bash
git clone https://github.com/True3DLabs/WindowMode.git
cd WindowMode
npm install
npm run dev
```

Open [http://localhost:3000](http://localhost:3000) with your browser to see the result.

## Project Structure

This is a NextJS project. The core funcionality driving the demo can be found at `components/WindowModeDemoPage.tsx`. We also use a small web worker for offloading tasks to a background thread, this can be found at `components/LandmarkWorker.tsx`. The file containing the 3D scene we render is stored at `public/target_visualization.vv`. `.vv` (voxel volume) is our file format for voxel-based 3D static scenes. More on this in "How do we render the scene."

## What is window mode?
Window mode is a 3D camera controller that emulates a window into the virtual world. You can imagine that your computer screen is really a portal into a 3D space.

It works by tracking the position of your face relative to the webcam, then re-rendering the 3D scene from the perspective of your face. This gives off the illusion that the 3D scene is really there, behind the screen, without the need for specialized hardware.

[more here](https://x.com/DannyHabibs/status/1973418113996861481)

It also works on any 3D video on [splats](https://www.splats.com/). Just click on the head icon in the player bar

## How does it work?
Here we use [MediaPipe](https://www.npmjs.com/package/@mediapipe/tasks-vision)'s `FaceLandmarker` system to extract the positions of the user's eyes. We use the apparent diameter of the eyes, along with the webcam's FOV in order to estimate the distance of the user's head from the webcam. We can then get an accurate estimate for the metric position of the user's eyes, relative to the webcam. 

Once we have the position of the users' face, we compute an *off-axis projection matrix*. This is a matrix transforming camera-relative coordinates to screen coordinates. It is what simulates the "portal" effect. This is done within our `spatial-player` library. For more information read [this article](https://en.wikibooks.org/wiki/Cg_Programming/Unity/Projection_for_Virtual_Reality). We will also be posting a video explainer to our [YouTube channel](https://www.youtube.com/@true3dlabs) soon.

## How do we render the scene?
All the rendering for this demo is done with our `spatial-player` library. You can install it on `npm` [here](https://www.npmjs.com/package/spatial-player). `spatial-player` is our framework for working with voxel-based 3D videos and static scenes. 

The targets are stored in a `.vv` (voxel volume) file. This is our file format for static, voxel-based 3D scenes. `spatial-player` also supports realtime rendering and playback of 3D volumetric videos, this is how our [Steamboat Willie Demo](https://www.splats.com/watch/702?window_mode=true&start_time=21) is rendered. Our volumetric videos are stored in `.splv` files.

### Using Your Own 3D Models

Want to use your own 3D artwork? You can easily convert any static GLB 3D model into a `.vv` file using our conversion tool:

**[Convert GLB to VV →](https://www.splats.com/tools/voxelize)**

Simply upload your GLB file (up to 500MB) and download the converted `.vv` file. Then replace the existing `.vv` files in the `public/` directory with your own!



You can render `.splv`s with `spatial-player`. If you want to create `.splv`s or `.vv`s to render, you should check out our python package `spatialstudio`. You can `pip` install it, check out the [documentation](https://pypi.org/project/spatialstudio/). If you have any questions/suggestions/requests for us or our stack, reach out to us on [discord](https://discord.gg/seBPMUGnhR).

Currently `spatial-player` and `spatialstudio` are only availble to install and use, but we will be open-sourcing them soon!

## Troubleshooting

### WebGPU Error
If you encounter an error related to WebGPU not being enabled, make sure you go to your browser's developer flags to enable it. This is required for the 3D rendering functionality.
</file>

</files>
